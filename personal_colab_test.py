# -*- coding: utf-8 -*-
"""Personal Colab Test.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/17cqXShsOkXt_Zela66_Sm_auzxs1KBts
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

league = pd.read_csv('https://raw.githubusercontent.com/jwcalder/MCFAM-Summer-School/master/Project%20Files/high_diamond_ranked_10min.csv')

league.columns

plt.figure()
sns.pairplot(league, vars=['blueKills', 'blueDeaths', 'blueAssists', 'blueGoldDiff', 'blueExperienceDiff'], hue = "blueWins")
plt.show()

import sklearn
from sklearn.metrics import *
from sklearn.svm import SVC
from sklearn.model_selection import *
import numpy as np
from numpy import loadtxt
import pandas as pd

league = pd.read_csv('https://raw.githubusercontent.com/jwcalder/MCFAM-Summer-School/master/Project%20Files/high_diamond_ranked_10min.csv')

features = ['blueKills', 'blueDeaths', 'blueAssists', 'blueGoldDiff', 'blueExperienceDiff']
X = league[features]
Y = league['blueWins']
X_train, X_test, y_train, y_test = train_test_split(X,Y,test_size = 0.99, random_state = 0)

model = SVC(max_iter = 100)
model.fit(X_train, y_train)

predictions = model.predict(X_test)
print(classification_report(y_test, predictions))
param_grid = {'C': [0.1, 1, 10, 100],
              'gamma': [1, 0.1, 0.01, 0.001, 0.0001], 
              'gamma':['scale', 'auto'],
              'kernel': ['linear', 'poly']}

grid = GridSearchCV(SVC(), param_grid, refit = True, verbose = 3,n_jobs=-1) 

# fitting the model for grid search 
grid.fit(X_train, y_train)  
 
# print best parameter after tuning 
print(grid.bestparams) 
grid_predictions = grid.predict(X_test) 

# print classification report 
print(classification_report(y_test, grid_predictions))

newLeague = league.dropna()

from sklearn.model_selection import train_test_split

from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import Pipeline

features = ['blueWardsPlaced', 'blueWardsDestroyed',
       'blueFirstBlood', 'blueKills', 'blueDeaths', 'blueAssists',
       'blueEliteMonsters', 'blueDragons', 'blueHeralds',
       'blueTowersDestroyed', 'blueTotalGold', 'blueAvgLevel',
       'blueTotalExperience', 'blueTotalMinionsKilled',
       'blueTotalJungleMinionsKilled', 'blueGoldDiff', 'blueExperienceDiff',
       'blueCSPerMin', 'blueGoldPerMin']

league_X = newLeague[features]
league_y = newLeague['blueWins']

league_X_train, league_X_test, league_y_train, league_y_test = train_test_split(league_X, league_y, test_size=0.01, random_state=14)

from sklearn import svm

svc = svm.SVC(kernel='linear', max_iter = -1)

scalar = StandardScaler()
pipeline = Pipeline([('transformer', scalar), ('estimator', svc)])

pipeline.fit(league_X_train, league_y_train)
blueWinPred = pipeline.predict(league_X_test)
from sklearn.metrics import *

print(random)
print(accuracy_score(league_y_test, blueWinPred))
print(precision_score(league_y_test, blueWinPred))
print(recall_score(league_y_test, blueWinPred))

newLeague = league.dropna()

from sklearn.model_selection import train_test_split

from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import Pipeline

features = ['blueWardsPlaced', 'blueWardsDestroyed',
       'blueFirstBlood', 'blueKills', 'blueDeaths', 'blueAssists',
       'blueEliteMonsters', 'blueDragons', 'blueHeralds',
       'blueTowersDestroyed', 'blueTotalGold', 'blueAvgLevel',
       'blueTotalExperience', 'blueTotalMinionsKilled',
       'blueTotalJungleMinionsKilled', 'blueGoldDiff', 'blueExperienceDiff',
       'blueCSPerMin', 'blueGoldPerMin']

league_X = newLeague[features]
league_y = newLeague['blueWins']

league_X_train, league_X_test, league_y_train, league_y_test = train_test_split(league_X, league_y, test_size=0.25, random_state=0)

from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import confusion_matrix, accuracy_score

classifier = KNeighborsClassifier(n_neighbors=179)
classifier.fit(league_X_train, league_y_train)

y_pred = classifier.predict(league_X_test)

accuracy = accuracy_score(league_y_test, y_pred)
print(accuracy)

k_list = list(range(101,200,2))
cv_scores = []

from sklearn.model_selection import StratifiedKFold
from sklearn.model_selection import cross_val_score

for k in k_list:
    knn = KNeighborsClassifier(n_neighbors=k)
    scores = cross_val_score(knn, league_X_train, league_y_train, cv = StratifiedKFold(shuffle=True), scoring = 'accuracy')
    cv_scores.append(scores.mean())

plt.figure()
plt.title('Performance of K Nearest Neighbors Algorithm')
plt.xlabel('Number of Neighbors K')
plt.ylabel('Accuracy Score')
plt.plot(k_list, cv_scores)

plt.show()

best_k = k_list[cv_scores.index(max(cv_scores))]
print(best_k)

newLeague = league.dropna()

from sklearn.model_selection import train_test_split

from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import Pipeline

features = ['blueWardsPlaced', 'blueWardsDestroyed',
       'blueFirstBlood', 'blueKills', 'blueDeaths', 'blueAssists',
       'blueEliteMonsters', 'blueDragons', 'blueHeralds',
       'blueTowersDestroyed', 'blueTotalGold', 'blueAvgLevel',
       'blueTotalExperience', 'blueTotalMinionsKilled',
       'blueTotalJungleMinionsKilled', 'blueGoldDiff', 'blueExperienceDiff',
       'blueCSPerMin', 'blueGoldPerMin']

league_X = newLeague[features]
league_y = newLeague['blueWins']

league_X_train, league_X_test, league_y_train, league_y_test = train_test_split(league_X, league_y, test_size=0.9, random_state=0)

from sklearn import tree

clf = tree.DecisionTreeClassifier()
clf = clf.fit(league_X_train, league_y_train)
y_pred = clf.predict(league_X_test)

from sklearn.ensemble import RandomForestClassifier
import graphviz

def fffffff():
  test = 0
  while test < 0.71:
    rf = RandomForestClassifier(n_estimators = 1000)
    rf = rf.fit(league_X_train, league_y_train)
    rf_pred = rf.predict(league_X_test)
    if accuracy_score(rf_pred, league_y_test) > test:
      test = accuracy_score(rf_pred, league_y_test)
      print(test)
  return rf

from sklearn.tree import export_graphviz

dot_data = tree.export_graphviz(fffffff().estimators_[0], out_file=None) 
graph = graphviz.Source(dot_data) 
graph

pip install mayavi

features = ['blueKills', 'blueDeaths', 'blueAssists',
       'blueGoldDiff', 'blueExperienceDiff']

from sklearn.neural_network import MLPClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import confusion_matrix, accuracy_score

X = league.loc[:, features].values
y = league.loc[:, 'blueWins'].values

scores = []
for i in range(20):
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=0)

    mlp = MLPClassifier(hidden_layer_sizes = (100,100,100,100,100,100,100), max_iter=2000)
    mlp.fit(X_train, y_train)
    
    y_pred = mlp.predict(X_test)
    accuracy = accuracy_score(y_test, y_pred)
    scores.append(accuracy)

print(np.average(scores))